= Glossary
:revdate: 2026-01-27
:page-revdate: {revdate}

== *cluster network*

Traffic-isolated forwarding path for transmission of network traffic in the {harvester-product-name} cluster.

== *guest cluster* / *guest Kubernetes cluster*

Group of integrated virtual machines that run on top of a {harvester-product-name} cluster.

You can create RKE2 and K3s guest clusters using the {harvester-product-name} and {rancher-product-name} interfaces. Creating guest clusters involves pulling images from either the internet or a private registry.

Guest clusters form the main infrastructure for running container workloads. Certain versions of {harvester-product-name} and {rancher-product-name} allow you to deploy container workloads xref:integrations/rancher/rancher-integration.adoc#_bare_metal_container_workload_support_experimental[directly to {harvester-product-name} clusters] (with some limitations).

== *guest node* / *guest cluster node*

Virtual machine that uses {harvester-product-name} cluster resources to run container workloads.

Guest nodes are managed through a control plane that controls pod-related activity and maintains the desired cluster state.

== *{harvester-product-name} cluster*

Group of integrated physical servers (hosts) on which the {harvester-product-name} hypervisor is installed. These servers collectively manage compute, memory, and storage resources to provide an environment for running virtual machines.

A three-node cluster is required to fully realize the multi-node features of {harvester-product-name}, particularly high availability. The latest versions allow you to create clusters with two management nodes and one xref:hosts/witness-node.adoc[witness node] (and optionally, one or more worker nodes). You can also create xref:installation-setup/single-node-clusters.adoc[single-node clusters] that support most features (excluding high availability, multi-replica support, and live migration).

{harvester-product-name} clusters can be imported into and managed by {rancher-product-name}. Within this context, an imported {harvester-product-name} cluster is known as a "managed cluster" or "downstream user cluster" (often abbreviated to "downstream cluster"). The term refers to any Kubernetes cluster that is connected to a {rancher-product-name} server.

Certain versions of {harvester-product-name} and {rancher-product-name} allow you to deploy container workloads directly to {harvester-product-name} clusters (with some limitations). When this xref:integrations/rancher/rancher-integration.adoc#_bare_metal_container_workload_support_experimental[experimental feature] is enabled, container workloads seamlessly interact with virtual machine workloads.

== *{harvester-product-name} hypervisor*

Specialized operating system and xref:introduction/overview.adoc#_architecture[software stack] that runs on a single physical server.

== *{harvester-product-name} ISO*

Installation image that contains the core operating system components and all required container images, which are preloaded during installation.

== *{harvester-product-name} node*

Physical server on which the {harvester-product-name} hypervisor is installed.

Each node that joins a {harvester-product-name} cluster must be assigned a xref:hosts/hosts.adoc#_role_management[role] that determines the functions the node can perform within the cluster. All {harvester-product-name} nodes process data but not all can store data.

== *Harvester Cloud Provider*

Component that enables a {rancher-short-name}-managed {harvester-product-name} cluster to function as a xref:integrations/rancher/cloud-provider.adoc[native cloud backend for guest Kubernetes clusters]. The Harvester Cloud Provider integrates the Kubernetes cloud controller manager (CCM) and container storage interface (CSI) driver, allowing guest clusters to dynamically request and manage {harvester-product-name}-native resources (load balancer and storage) without manual intervention.

== *Harvester CSI Driver*

Driver that xref:integrations/rancher/csi-driver.adoc[provides a standard container storage interface (CSI) for guest Kubernetes clusters], allowing container workloads to use the storage solution integrated with the underlying {harvester-product-name} cluster. This component enables hotplugging of volumes to the virtual machines to provide native storage performance.

== *Harvester Node Driver*

Driver that {rancher-product-name} uses to xref:integrations/rancher/node-driver/node-driver.adoc[provision virtual machines in a {harvester-product-name} cluster], and to launch and manage guest Kubernetes clusters on top of those virtual machines.

== *live migration*

Process of xref:virtual-machines/live-migration.adoc[moving a running virtual machine to another node] within the same {harvester-product-name} cluster without interrupting the guest operating system and causing workload downtime. Live migration can occur only when the xref:virtual-machines/live-migration.adoc#_prerequisites[prerequisites] are met and when the affected virtual machines are xref:virtual-machines/live-migration.adoc#_live_migratable_virtual_machines[live-migratable].

== *mgmt*

Cluster network that is xref:networking/cluster-network.adoc#_built_in_cluster_network[automatically created] during {harvester-product-name} cluster deployment and is always enabled on all hosts. {harvester-product-name} uses `mgmt` for intra-cluster communications and cluster management tasks.

== *net install ISO*

Installation image that contains only the xref:installation-setup/media/net-install.adoc[core {harvester-product-name} operating system components], allowing the installer to boot and then install the operating system on a disk. After installation is completed, the operating system pulls all required container images from the internet.

== *network configuration*

Definition of how a set of cluster nodes with uniform network specifications connects to a specific cluster network.

== *overlay network*

Virtual network representing a virtual layer 2 switch that xref:networking/harvester-network.adoc#_overlay_network_experimental[encapsulates and forwards traffic between virtual machines]. Overlay networks support advanced software-defined networking (SDN) capabilities such as virtual private clouds (VPCs) and subnets for virtual machine workloads.

== *storage network*

Network for xref:storage/storage-network.adoc[isolating Longhorn replication traffic] from intra-cluster traffic on `mgmt` and other cluster-wide workloads.

== *VLAN trunk network*

Virtual network that provides a virtual machine with a single network interface that can carry traffic for xref:networking/harvester-network.adoc#_vlan_trunk_network[multiple, overlapping VLAN ID ranges] simultaneously. When a virtual machine is attached to a VLAN trunk network, the guest operating system and applications are allowed to send and receive packets tagged with any of the VLAN IDs within the specified range.

== *VM migration network*

Network for xref:storage/vm-migration-network.adoc[isolating virtual machine migration traffic] from intra-cluster traffic on `mgmt` and other cluster-wide workloads.

== *VM network*

Virtual network linked to a specific cluster network that enables communication between virtual machines and the external network.

== *witness node*

xref:hosts/witness-node.adoc[Non-computing node] used solely to maintain cluster consensus. It ensures the system can reach a majority decision (quorum) on cluster updates even if a management node becomes unavailable or a network failure occurs.

Witness nodes do not run workloads and store data. Each {harvester-product-name} cluster can have only one witness node.