= Upgrades
:revdate: 2025-12-26
:page-revdate: {revdate}

{harvester-product-name} is adopting a new lifecycle strategy that simplifies version management and upgrades. This strategy includes the following:

* Four-month minor release cadence
* Two-month patch release cadence
* Component adoption policy

[NOTE]
====
{harvester-product-name} does not support downgrades. This restriction helps prevent unexpected system behavior and issues associated with function incompatibility, deprecation, and removal.
====

== Upgrade paths

The following table outlines the supported upgrade paths.

|===
| Installed Version | Supported Upgrade Versions

| v1.6.x
| xref:./v1-6-x-to-v1-7-x.adoc[v1.7.x]

| v1.6.x
| xref:./v1-6-x-to-v1-6-y.adoc[v1.6.y] (_y_ is greater than _x_)

| v1.5.x
| xref:./v1-5-x-to-v1-6-x.adoc[v1.6.x]

| v1.5.0 and v1.5.1
| xref:./v1-5-0-to-v1-5-2.adoc[v1.5.2]

| v1.5.0
| xref:./v1-5-0-to-v1-5-1.adoc[v1.5.1]

| v1.4.2 and v1.4.3
| xref:./v1-4-2-to-v1-5-2.adoc[v1.5.2]

| v1.4.2 and v1.4.3
| xref:./v1-4-2-to-v1-5-0.adoc[v1.5.0] and xref:./v1-4-2-to-v1-5-1.adoc[v1.5.1]

| v1.4.1 and v1.4.2
| xref:./v1-4-1-to-v1-4-3.adoc[v1.4.3]

| v1.4.1
| xref:./v1-4-1-to-v1-4-2.adoc[v1.4.2]

| v1.4.0
| xref:./v1-4-0-to-v1-4-1.adoc[v1.4.1]

| v1.3.1
| xref:./v1-3-1-to-v1-3-2.adoc[v1.3.2]

| v1.2.2 and v1.3.0
| xref:./v1-2-2-to-v1-3-1.adoc[v1.3.1]

| v1.2.1
| xref:./v1-2-1-to-v1-2-2.adoc[v1.2.2]

| v1.1.2, v1.1.3, and v1.2.0
| xref:./v1-2-0-to-v1-2-1.adoc[v1.2.1]
|===

The latest {harvester-product-name} versions allow the following:

* Upgrading from one minor version to the next (for example, from v1.5.2 to v1.6.1) without needing to install the patches released in between the two versions. This is possible because {harvester-product-name} allows a maximum of one minor version upgrade for underlying components.
* Upgrading to a later patch version (for example, from v1.6.0 to v1.6.1), assuming that the same component versions are used across the releases for a given minor version.

The following table outlines the components used in these versions:

|===
| Component | {harvester-product-name} v1.5.x | {harvester-product-name} v1.6.x | {harvester-product-name} v1.7.x

| KubeVirt
| v1.4
| v1.5
| v1.6

| {longhorn-product-name}
| v1.8
| v1.9
| v1.10

| {rancher-product-name}
| v2.11
| v2.12
| v2.13

| RKE2
| v1.32
| v1.33
| v1.34
|===

[NOTE]
====
Skipping multiple Kubernetes minor versions is not supported upstream and is a key reason behind the limited upgrade paths. For more information, see https://kubernetes.io/releases/version-skew-policy[Version Skew Policy] in the Kubernetes documentation.
====

== {rancher-short-name} upgrade

If you are using {rancher-short-name} to manage your {harvester-product-name} cluster, you must https://documentation.suse.com/cloudnative/rancher-manager/v2.11/en/installation-and-upgrade/upgrades.html[upgrade {rancher-short-name}] _before_ upgrading {harvester-product-name}.

[IMPORTANT]
====
The {harvester-product-name} and {rancher-short-name} upgrade processes are independent of each other. During a {rancher-short-name} upgrade, you can still access your {harvester-product-name} cluster using its virtual IP. {harvester-product-name} is not automatically upgraded.
====

When a {rancher-short-name} version reaches its End of Maintenance (EOM) date, {harvester-product-name} only provides fixes for critical security-related issues that affect integration functions (Virtualization Management). For more information, see the https://www.suse.com/suse-harvester/support-matrix/all-supported-versions/[Support Matrix].

== Virtual Machine Management through the Upgrade

=== Live-Migratable Virtual Machines

xref:virtual-machines/live-migration.adoc#_live_migratable_virtual_machines[Live-migratable virtual machines] are automatically migrated to other nodes via xref:../virtual-machines/live-migration.adoc#_automatically_triggered_batch_migration[batch migration] before the current node is upgraded. These virtual machines experience zero downtime during migration.

=== Non-Migratable Virtual Machines

When an upgrade is triggered, {harvester-product-name} performs certain actions depending on the value of the xref:installation-setup/config/settings.adoc#_upgrade_config[`upgrade-config`] setting's `restoreVM` option.

* `false`: {harvester-product-name} does not perform the upgrade when xref:virtual-machines/live-migration.adoc#_non_migratable_virtual_machines[non-migratable virtual machines] are still running. You must manually power off the virtual machines.

* `true`: {harvester-product-name} automatically powers off non-migratable virtual machines when the node is upgraded and then restores them after the node is rebooted.

[CAUTION]
====
Non-migratable virtual machines experience downtime during migration.
====

For more information, see xref:upgrades/troubleshooting.adoc#_phase_4_upgrade_nodes[Phase 4: Upgrade Nodes].

== Before starting an upgrade

Check out the available xref:../installation-setup/config/settings.adoc#_upgrade_config[`upgrade-config` setting] to tweak the upgrade strategies and behaviors that best suit your cluster environment.

== Start an upgrade

[CAUTION]
====
* Before you upgrade your {harvester-product-name} cluster, we highly recommend:
 ** Back up your VMs if needed.
* Do not operate the cluster during an upgrade. For example, creating new VMs, uploading new images, etc.
* Make sure your hardware meets the *preferred* xref:../installation-setup/requirements.adoc#_hardware_requirements[hardware requirements]. This is due to there will be intermediate resources consumed by an upgrade.
* Make sure each node has at least 30 GiB of free system partition space (`df -h /usr/local/`). If any node in the cluster has less than 30 GiB of free system partition space, the upgrade will be denied. Check <<Free system partition space requirement,free system partition space requirement>> for more information.
* Run the pre-check script on a {harvester-product-name} control-plane node. Please pick a script according to your cluster's version: https://github.com/harvester/upgrade-helpers/tree/main/pre-check.
* A number of one-off privileged pods will be created in the `harvester-system` and `cattle-system` namespaces to perform host-level upgrade operations. If https://kubernetes.io/docs/concepts/security/pod-security-admission/[pod security admission] is enabled, adjust these policies to allow these pods to run.
====

[CAUTION]
====
* Make sure all nodes' times are in sync. Using an NTP server to synchronize time is recommended. If an NTP server is not configured during the installation, you can manually add an NTP server *on each node*:
+
[,sh]
----
  $ sudo -i

  # Add time servers
  $ vim /etc/systemd/timesyncd.conf
  [ntp]
  NTP=0.pool.ntp.org

  # Enable and start the systemd-timesyncd
  $ timedatectl set-ntp true

  # Check status
  $ sudo timedatectl status
----
====

[CAUTION]
====
* NICs that connect to a PCI bridge might be renamed after an upgrade. Please check the https://harvesterhci.io/kb/nic-naming-scheme[knowledge base article] for further information.
====

. On the {harvester-product-name} UI *Dashboard* screen, click *Upgrade*.
+
The *Upgrade* button appears whenever a new version that you can upgrade to becomes available.
+
If your environment does not have direct internet access, follow the instructions in <<Prepare an air-gapped upgrade>>, which provides an efficient approach to downloading the ISO.
+
image::upgrade/upgrade_button.png[]

. Select the version that you want to upgrade to.
+
If you require customizations, see <<Customize the version>>.
+
image::upgrade/upgrade_select_version.png[]

. Click the progress indicator (circular icon) to view the status of each related process.
+
image::upgrade/upgrade_progress.png[]

=== Customize the version

. Download the version file (`pass:[https://releases.rancher.com/harvester/{version}/version.yaml]`).
+
Example:
+
The https://releases.rancher.com/harvester/v1.5.0/version.yaml[v1.5.0 version file] is downloaded as `v1.5.0.yaml`.
+
[,yaml]
----
apiVersion: harvesterhci.io/v1beta1
kind: Version
metadata:
  name: v1.5.0-customized # Changed, to avoid duplicated with the official version name
  namespace: harvester-system
spec:
  isoChecksum: 'df28e9bf8dc561c5c26dee535046117906581296d633eb2988e4f68390a281b6856a5a0bd2e4b5b988c695a53d0fc86e4e3965f19957682b74317109b1d2fe32'  # Don't change
  isoURL: https://releases.rancher.com/harvester/v1.5.0/harvester-v1.5.0-amd64.iso # Official ISO path by default
  releaseDate: '20250425'
----

. Create the version using the command `kubectl create -f v1.5.0.yaml`.

== Prepare an air-gapped upgrade

[CAUTION]
====
Make sure to check <<Upgrade paths>> section first about upgradable versions.
====

=== Prepare the ISO file

. Download an ISO file from the https://github.com/harvester/harvester/releases[Releases] page.

. Save the ISO to a local HTTP server.
+
Assume the file is hosted at `http://10.10.0.1/harvester.iso`.

=== Prepare the Version

. Download the version file (`pass:[https://releases.rancher.com/harvester/{version}/version.yaml]`).

. Replace the `isoURL` value in the file.
+
[,yaml]
----
  apiVersion: harvesterhci.io/v1beta1
  kind: Version
  metadata:
    name: v1.5.0
    namespace: harvester-system
  spec:
    isoChecksum: <SHA-512 checksum of the ISO>
    isoURL: http://10.10.0.1/harvester.iso  # change to local ISO URL
    releaseDate: '20250425'
----
+
Assume the file is hosted at `http://10.10.0.1/version.yaml`. If you require customizations, see <<Customize the version>>.

. Access one of the control plane nodes via SSH and log in using the root account.

. Create a version object.
+
[,console]
----
rancher@node1:~> sudo -i
rancher@node1:~> kubectl create -f http://10.10.0.1/version.yaml
----

=== Start the upgrade

The *Upgrade* button appears on the *Dashboard* screen whenever a new version that you can upgrade to becomes available. Refresh the screen if the button does not appear.

== Manually start an upgrade before the official upgrade becomes available

The *Upgrade* button does not appear on the UI immediately after a new version is released. If you want to upgrade your cluster before the option becomes available on the UI, follow the steps in <<Prepare an air-gapped upgrade>>.

[TIP]
====
In production environments, upgrading clusters via the UI is recommended.
====

== Free system partition space requirement

{harvester-product-name} loads images on each node during upgrades. When disk usage exceeds the kubelet's garbage collection threshold, the kubelet deletes unused images to free up space. This may cause issues in air-gapped environments because the images are not available on the node.

{harvester-product-name} includes checks that ensure nodes do not trigger garbage collection after loading new images.

When disk space is insufficient, {harvester-product-name} blocks the upgrade and returns an error similar to the following:

[,console]
----
Node "harvester-node-0" will reach 92.84% storage space after loading new images. It's higher than kubelet image garbage collection threshold 85%.
----

If you want to try upgrading even if the free system partition space is insufficient on some nodes, you can update the `harvesterhci.io/skipGarbageCollectionThresholdCheck: true` annotation of the `Upgrade` object.

[,yaml]
----
apiVersion: harvesterhci.io/v1beta1
kind: Upgrade
metadata:
  annotations:
    harvesterhci.io/skipGarbageCollectionThresholdCheck: true
  generateName: hvst-upgrade-
  namespace: harvester-system
spec:
  version: "1.6.0"
  logEnabled: true
----

[CAUTION]
====
Setting a smaller value than the pre-defined value may cause the upgrade to fail and is not recommended in a production environment.
====

The following sections describe solutions for issues related to this requirement.

=== Free system partition space manually

{harvester-product-name} attempts to remove unnecessary container images after an upgrade is completed. However, this automatic image cleanup may not be performed for various reasons. You can use https://github.com/harvester/upgrade-helpers/blob/main/bin/harv-purge-images.sh[a script] to manually remove images. For more information, see issue https://github.com/harvester/harvester/issues/6620[#6620].

=== Set up a private container registry and skip image preloading

The system partition might still lack free space even after you remove images. To address this, set up a private container registry for both current and new images, and configure the setting xref:../installation-setup/config/settings.adoc#_upgrade_config[`upgrade-config`] with following value:

[,json]
----
{"imagePreloadOption":{"strategy":{"type":"skip"}}, "restoreVM": false}
----

{harvester-product-name} skips the upgrade image preloading process. When the deployments on the nodes are upgraded, the container runtime loads the images stored in the private container registry.

[CAUTION]
====
Do not rely on the public container registry. Note any potential internet service interruptions and how close you are to reaching your https://www.docker.com/increase-rate-limits[Docker Hub rate limit]. Failure to download any of the required images may cause the upgrade to fail and may leave the cluster in a middle state.
====

== Certificate expiration check

{harvester-product-name} checks the validity period of certificates on each node. This check eliminates the possibility of certificates expiring while the upgrade is in progress. If a certificate will expire within 7 days, an error is returned. This behavior can be overridden by setting the `harvesterhci.io/minCertsExpirationInDay` annotation.

Example:

[,yaml]
----
apiVersion: harvesterhci.io/v1beta1
kind: Upgrade
metadata:
  annotations:
    harvesterhci.io/minCertsExpirationInDay: "14"
  generateName: hvst-upgrade-
  namespace: harvester-system
spec:
  version: "1.6.0"
  logEnabled: true
----

When this annotation is added to the `Upgrade` object, {harvester-product-name} returns an error when it detects a certificate that will expire within 14 days.

For more information, see xref:installation-setup/config/settings.adoc#_auto_rotate_rke2_certs[auto-rotate-rke2-certs].

== Virtual Machine Backup Compatibility

You may encounter certain limitations when creating and restoring backups that involve external storage.

== Longhorn Manager Crashes Due to Backing Image Eviction

[CAUTION]
====
When upgrading to {harvester-product-name} *v1.4.x*, Longhorn Manager may crash if the `EvictionRequested` flag is set to `true` on any node or disk. This issue is caused by a https://longhorn.io/kb/troubleshooting-longhorn-manager-crashes-due-to-backing-image-eviction/[race condition] between the deletion of a disk in the backing image spec and the updating of its status.

To prevent the issue from occurring, ensure that the `EvictionRequested` flag is set to `false` before you start the upgrade process.
====

== Re-enable RKE2 ingress-nginx admission webhooks (CVE-2025-1974)

If you https://harvesterhci.io/kb/2025/03/25/cve-2025-1974[disabled the RKE2 ingress-nginx admission webhooks] to mitigate https://nvd.nist.gov/vuln/detail/CVE-2025-1974[CVE-2025-1974], you must re-enable the webhook after upgrading to {harvester-product-name} v1.5.0 or later.

. Verify that {harvester-product-name} is using nginx-ingress v1.12.1 or later.
+
[,shell]
----
$ kubectl -n kube-system get po -l"app.kubernetes.io/name=rke2-ingress-nginx" -ojsonpath='{.items[].spec.containers[].image}'
rancher/nginx-ingress-controller:v1.12.1-hardened1
----

. Run `kubectl -n kube-system edit helmchartconfig rke2-ingress-nginx` to *remove* the following configurations from the `HelmChartConfig` resource.
+
* `.spec.valuesContent.controller.admissionWebhooks.enabled: false`
* `.spec.valuesContent.controller.extraArgs.enable-annotation-validation: true`

. Verify that the new `.spec.ValuesContent` configuration is similar to the following example.
+
[,yaml]
----
apiVersion: helm.cattle.io/v1
kind: HelmChartConfig
metadata:
  name: rke2-ingress-nginx
  namespace: kube-system
spec:
  valuesContent: |-
    controller:
      admissionWebhooks:
        port: 8444
      extraArgs:
        default-ssl-certificate: cattle-system/tls-rancher-internal
      config:
        proxy-body-size: "0"
        proxy-request-buffering: "off"
      publishService:
        pathOverride: kube-system/ingress-expose
----
+
[IMPORTANT]
====
If the `HelmChartConfig` resource contains other custom `ingress-nginx` configuration, you must retain them when editing the resource.
====

. Exit the `kubectl edit` command execution to save the configuration.
+
{harvester-product-name} automatically applies the change once the content is saved.

. Verify that the `rke2-ingress-nginx-admission` webhook configuration is re-enabled.
+
[,shell]
----
$ kubectl get validatingwebhookconfiguration rke2-ingress-nginx-admission
NAME                           WEBHOOKS   AGE
rke2-ingress-nginx-admission   1          6s
----

. Verify that the `ingress-nginx` pods are restarted successfully.
+
[,shell]
----
kubectl -n kube-system get po -lapp.kubernetes.io/instance=rke2-ingress-nginx
NAME                                  READY   STATUS    RESTARTS   AGE
rke2-ingress-nginx-controller-l2cxz   1/1     Running   0          94s
----

=== Upgrade is stuck in the "Pre-drained" state

The upgrade process may become stuck in the "Pre-drained" state. Kubernetes is supposed to drain the workload on the node, but some factors may cause the process to stall.

image::upgrade/3730-stuck.png[]

A possible cause is processes related to orphan engines of the Longhorn Instance Manager. To determine if this applies to your situation, perform the following steps:

. Check the name of the `instance-manager` pod on the stuck node.
+
Example:
+
The stuck node is `harvester-node-1`, and the name of the Instance Manager pod is `instance-manager-d80e13f520e7b952f4b7593fc1883e2a`.
+
[,shell]
----
$ kubectl get pods -n longhorn-system --field-selector spec.nodeName=harvester-node-1 | grep instance-manager
instance-manager-d80e13f520e7b952f4b7593fc1883e2a          1/1     Running   0              3d8h
----

. Check the Longhorn Manager logs for informational messages.
+
Example:
+
[,shell]
----
$ kubectl -n longhorn-system logs daemonsets/longhorn-manager
...
time="2025-01-14T00:00:01Z" level=info msg="Node instance-manager-d80e13f520e7b952f4b7593fc1883e2a is marked unschedulable but removing harvester-node-1 PDB is blocked: some volumes are still attached InstanceEngines count 1 pvc-9ae0e9a5-a630-4f0c-98cc-b14893c74f9e-e-0" func="controller.(*InstanceManagerController).syncInstanceManagerPDB" file="instance_manager_controller.go:823" controller=longhorn-instance-manager node=harvester-node-1
----
+
The `instance-manager` pod cannot be drained because of the engine `pvc-9ae0e9a5-a630-4f0c-98cc-b14893c74f9e-e-0`.

. Check if the engine is still running on the stuck node.
+
Example:
+
[,shell]
----
$ kubectl -n longhorn-system get engines.longhorn.io pvc-9ae0e9a5-a630-4f0c-98cc-b14893c74f9e-e-0 -o jsonpath='{"Current state: "}{.status.currentState}{"\nNode ID: "}{.spec.nodeID}{"\n"}'
Current state: stopped
Node ID:
----
+
The issue likely exists if the output shows that the engine is either not running or not found.

. Check if all volumes are healthy.
+
[,shell]
----
kubectl get volumes -n longhorn-system -o yaml | yq '.items[] | select(.status.state == "attached")| .status.robustness'
----
+
All volumes must be marked `healthy`. If this is not the case, report the issue.

. Remove the `instance-manager` pod's PodDisruptionBudget (PDB).
+
Example:
+
[,shell]
----
kubectl delete pdb instance-manager-d80e13f520e7b952f4b7593fc1883e2a -n longhorn-system
----

Related issues:

* {harvester-product-name}: https://github.com/harvester/harvester/issues/7366[#7366]
* {longhorn-product-name}: https://github.com/longhorn/longhorn/issues/6764[#6764] and https://github.com/longhorn/longhorn/issues/11605[#11605]

== Recurring {longhorn-product-name} snapshots and backups are unsupported

https://longhorn.io/docs/1.10.0/snapshots-and-backups/scheduling-backups-and-snapshots[Recurring {longhorn-product-name} snapshots and backups] are not integrated into {harvester-product-name}. If you decide to use this feature, you must disable all *recurring snapshot and backup jobs in {longhorn-product-name}* before starting the upgrade.

For more information about the incompatibility, see xref:virtual-machines/backup-restore.adoc#_scheduled_virtual_machine_backups_and_snapshots[Scheduled virtual machine backups and snapshots].